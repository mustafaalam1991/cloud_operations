# ğŸ“˜ Ceph Administration and Monitoring Guide

A practical reference for managing Ceph in CBIS environments. Covers common administrative tasks, cluster performance monitoring, and troubleshooting.

---

## ğŸ“š Table of Contents

1. [Getting Started](#getting-started)
2. [Ceph Administration Tasks](#ceph-administration-tasks)

   * [1. Changing Replication Factor](#1-changing-replication-factor)
   * [2. Calculating PG Numbers](#2-calculating-pg-numbers)
   * [3. Modifying PG Values](#3-modifying-pg-values)
   * [4. Pool Details](#4-pool-details)
   * [5. Mapping OSDs to Disks](#5-mapping-osds-to-disks)
   * [6. Restarting an OSD](#6-restarting-an-osd)
   * [7. Admin Sockets (asok)](#7-admin-sockets-asok)
   * [8. Reverting Lost Objects](#8-reverting-lost-objects)
   * [9. Scrubbing PGs](#9-scrubbing-pgs)
   * [10. High Disk Utilization OSDs](#10-high-disk-utilization-osds)
3. [Ceph Monitoring](#ceph-monitoring)

   * [Health](#health)
   * [Cluster Usage](#cluster-usage)
   * [OSD Stats](#osd-stats)
   * [Performance Stats](#performance-stats)
4. [Useful References](#useful-references)

---

## ğŸ Getting Started

To begin learning Ceph administration, start with the [10 Commands Every Ceph Administrator Should Know](https://tracker.ceph.com/projects/ceph/wiki/10_Commands_Every_Ceph_Administrator_Should_Know).

---

## âš™ï¸ Ceph Administration Tasks

### 1. Changing Replication Factor

By default, Ceph pools are set with a replication factor of 3. VNF owners may request a replication factor of 2 to improve I/O performance. Lowering the replication factor reduces redundancy and increases the risk of data loss.

**Steps:**

```bash
# Check current replication settings
sudo ceph osd pool ls detail

# Watch Ceph logs in a new terminal
sudo ceph -w

# Change replication size
sudo ceph osd pool set <pool-name> size 2
```

**Pools of Interest:**

* `volumes`: VMs on SSDs
* `volumes-fast`: VMs on NVMe

---

### 2. Calculating PG Numbers

The number of placement groups (PGs) affects performance. Use the formula:

```
Total PGs = (Number of OSDs Ã— 100) Ã· Replication Factor
```

**Examples:**

* 80 OSDs, size 2 â†’ 4000 â†’ Use 4096 (nearest power of 2)
* 80 OSDs, size 3 â†’ 2667 â†’ Use 2048

ğŸ”— [Placement Group Sizing Guide](https://docs.ceph.com/docs/mimic/rados/operations/placement-groups/)

---

### 3. Modifying PG Values

Check and modify PG values for a pool.

```bash
# View current PG values
sudo ceph osd pool get <pool-name> pg_num
sudo ceph osd pool get <pool-name> pgp_num

# Set new values
sudo ceph osd pool set <pool-name> pg_num <value>
sudo ceph osd pool set <pool-name> pgp_num <value>
```

> ğŸ“Œ Note: Always set `pgp_num = pg_num` for effective data balancing via CRUSH.

---

### 4. Pool Details

```bash
# List detailed pool info, including replication factor
sudo ceph osd pool ls detail
```

---

### 5. Mapping OSDs to Disks

To identify a failed disk associated with an OSD:

```bash
# Identify OSDs status
sudo ceph osd tree

# Map to physical disk
lsblk
```

---

### 6. Restarting an OSD

```bash
# Restart OSD with ID 13 (example)
sudo systemctl restart ceph-osd@13.service
```

ğŸ”— [Troubleshooting Stuck PGs (Red Hat)](https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/2/html/troubleshooting_guide/troubleshooting-osds#slow-requests-and-requests-are-blocked)

---

### 7. Admin Sockets (asok)

Admin sockets offer fine-grained OSD information.

```bash
# Query OSD config
ceph --admin-daemon /var/run/ceph/ceph-osd.0.asok config show | grep <parameter>
```

Example:

```bash
ceph --admin-daemon /var/run/ceph/ceph-osd.0.asok config show | grep osd_max_backfills
```

ğŸ”— [Admin Sockets Guide (Red Hat)](https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/2/html-single/administration_guide/index#using_the_administration_socket)

---

### 8. Reverting Lost Objects

If objects are permanently lost, mark them as such:

```bash
ceph pg <pg-id> mark_unfound_lost revert
```

> Only `revert` is supported. Use with caution.

---

### 9. Scrubbing PGs

Automatically repair inconsistent PGs:

```bash
ceph pg dump | grep -i incons | cut -f1 -d" " | while read i; do ceph pg repair $i; done
```

---

### 10. High Disk Utilization OSDs

```bash
ceph osd df plain | sort -rn -k 7
```

---

## ğŸ“Š Ceph Monitoring

### Health

```bash
# Cluster health status
ceph -s -f json-pretty | grep "overall_status" | awk '{print $2}'

# Monitors in quorum
ceph mon stat | awk '{print $2}'

# Pools count
ceph osd pool ls | wc -l

# Live Ceph log
ceph -w
```

---

### Cluster Usage

```bash
# Total capacity
ceph osd df -f json-pretty | grep "total_kb" | head -n1 | awk '{print $2}'

# Used capacity
ceph osd df -f json-pretty | grep "total_kb_used" | awk '{print $2}'

# Available capacity
ceph osd df -f json-pretty | grep "total_kb_avail" | awk '{print $2}'

# Total objects
ceph df detail -f json-pretty | grep "total_objects" | awk '{print $2}'

# Average utilization
ceph osd df -f json-pretty | grep "average_utilization" | awk '{print $2}'
```

---

### OSD Stats

```bash
# OSD states
ceph -s -f json-pretty | grep "num_in_osds" | head -n1 | awk '{print $2}'
ceph -s -f json-pretty | grep "num_up_osds" | head -n1 | awk '{print $2}'
sudo ceph osd tree | grep down | wc -l  # OSDs down

# PGs per OSD
ceph osd utilization | head -n1 | awk '{print $2}'
```

---

### Performance Stats

```bash
# Throughput
ceph -s -f json-pretty | grep "read_bytes_sec" | awk '{print $2}'
ceph -s -f json-pretty | grep "write_bytes_sec" | awk '{print $2}'

# Operations per second
ceph -s -f json-pretty | grep "read_op_per_sec" | awk '{print $2}'
ceph -s -f json-pretty | grep "write_op_per_sec" | awk '{print $2}'
```

---

## ğŸ”— Useful References

* ğŸ“˜ [Ceph Commands Guide](https://tracker.ceph.com/projects/ceph/wiki/10_Commands_Every_Ceph_Administrator_Should_Know)
* ğŸ“˜ [Ceph Admin Wiki](https://tracker.ceph.com/projects/ceph/wiki/Guides)
* ğŸ“˜ [Placement Groups Documentation](https://docs.ceph.com/docs/mimic/rados/operations/placement-groups/)
* ğŸ“˜ [Red Hat Admin Sockets Guide](https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/2/html-single/administration_guide/index#using_the_administration_socket)
* ğŸ“˜ [Red Hat Troubleshooting Guide](https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/2/html/troubleshooting_guide/)
