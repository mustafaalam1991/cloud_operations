++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Ceph Administration:
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
There are many online resources that are related to Administring Ceph. To get you started you can visit:
https://tracker.ceph.com/projects/ceph/wiki/10_Commands_Every_Ceph_Administrator_Should_Know



1. Change replication factor on the pools:
--------------------------------------------------------------------------------------------------------------------
The default replication size of all the pools in storage (ceph) on CBIS is set to 3. The need to changing the replication factor could 
arise due to I/O performance requirements from the VNF owners, for which the replication factor of 2 is preferred. Note that this comes with 
a cost. When you decrease the replication factor, it means you have lesser copies of data lying on the storage nodes. Hence chances of data
loss are increased. But also note that Ceph is a Highly resilient/fault tolerant distributed Data Storage system that ensures the integrity 
of data incase of failures of disk/nodes. 

	- First check the current replication factor of the respective pool
		$ sudo ceph osd pool ls detail
	- Identify which pool's replication factor you need to change:
		- Usually there are two pools that store VM data
			-> volumes - Contains VMs block devices - This is based on SSD OSDs
			-> volumes-fast - Also contains VM block devices - This is based on nVME OSDs. 
	- It's a good practice to monitor ceph logs during any changes made to ceph storage. Run the following command in a new tab
		- $ sudo ceph -w
	- Now change the replication factor, by running the below command: 
		- $ sudo ceph osd pool set <pool-name> size 2

2. Calculating pg_num and pgp_num of a pool:
------------------------------------------------------------------------------------------------------------------
A general guideline of setting placement groups (pg_num and pgp_num) is provided in the Placement Group documentation: 
https://docs.ceph.com/docs/mimic/rados/operations/placement-groups/
		
	-                      (OSDs * 100)
		Total PGs =    ------------
           		         pool size
	- Some examples are below:
		- e.g. for a replication factor of 2 and # of osds = 80
			Total PGs =  4000 (4096 - nearest power of 2)
		- e.g. for a replication factor of 3 and # of osds = 80
			Total PGs =  2667 (2048 - nearest power of 2)
	- Increasing PGs of a pool will increase I/O performance at the cost of CPU and memory of the storage node. When calculating PGs the physical 
resources of the storage node (server) should also be taken into consideration. 



3. Changing the pg_num and pgp_num of the pools:
------------------------------------------------------------------------------------------------------------------
 	- Check the current pg_num and pgp_num of the pool:
		- $sudo ceph osd pool get {pool-name} pg_num
		- $sudo ceph osd pool get {pool-name} pgp_num
	- pg_num and pgp_num can be set by running the following commands: 
		- $ceph osd pool set {pool-name} pg_num {pg_num}
		- $ceph osd pool set {pool-name} pgp_num {pgp_num}

Once you increase the number of placement groups, you must also increase the number of placement groups for placement (pgp_num) before your cluster will 
rebalance. The pgp_num will be the number of placement groups that will be considered for placement by the CRUSH algorithm. Increasing pg_num splits the 
placement groups but data will not be migrated to the newer placement groups until placement groups for placement, ie. pgp_num is increased. The pgp_num 
should be equal to the pg_num. 

Further Reading: https://docs.ceph.com/docs/mimic/rados/operations/placement-groups/

	
4. Find out pool details:
------------------------------------------------------------------------------------------------------------------	
	- Following command is very useful to get a full picture of the pools available in ceph (inclusing the replication factor): 
		- $sudo ceph osd pool ls detail



5. Map osds to disk - How to identify a faulty osd disk?
------------------------------------------------------------------------------------------------------------------	
Often times you have a faulty OSDs that needs replacement
	- Identify the osd number (osd id) that is faulty. Simply run to check which osd is down: 
			$ sudo ceph osd tree 
	- Now run the following command to get the /dev/sd$x (drive letter) of the faulty osds:
			$ lsblk
Remember these steps are just to identify the drive of the faulty osds.  

6. Restart an OSD: 
------------------------------------------------------------------------------------------------------------------
	$ systemctl restart ceph-osd@13.service

Refer to these links in case you have pg stuck of operations stuck:
https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/2/html/troubleshooting_guide/troubleshooting-osds#slow-requests-and-requests-are-blocked

7. ceph admin sockets (asok) have very fine grained information of OSD state:
------------------------------------------------------------------------------------------------------------------
	$ ceph --admin-daemon ceph-osd.0.asok config show |grep <param_of_interest>
	ex: ceph --admin-daemon ceph-osd.0.asok config show |grep osd_max_backfills


Refer to this link for documentation on admin sockets:
https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/2/html-single/administration_guide/index#using_the_administration_socket

8. Revert Lost
------------------------------------------------------------------------------------------------------------------
If the cluster has lost one or more objects, and you have decided to abandon the search for the lost data, you must mark the unfound objects as lost.

If all possible locations have been queried and objects are still lost, you may have to give up on the lost objects. This is possible given unusual combinations of failures that allow the cluster to learn about writes that were performed before the writes themselves are recovered.

Currently the only supported option is “revert”, which will either roll back to a previous version of the object or (if it was a new object) forget about it entirely. To mark the “unfound” objects as “lost”, execute the following:

ceph pg {pg-id} mark_unfound_lost revert|delete

9. PG scrubbing
------------------------------------------------------------------------------------------------------------------
ceph pg dump | grep -i incons | cut -f1 -d" " | while read i; do ceph pg repair ${i} ; done



++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Ceph Monitoring: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

						Ceph Health:
==========================================================================================================
ceph health status 	- 	ceph -s -f json-pretty | grep "overall_status"|awk '{print $2}'
# of monitors in qorum 	-	ceph mon stat|awk '{print $2}'
# of pools available 	-	ceph osd pool ls|wc -l
Monitor ceph logs	-	ceph -w

						Cluster Usage: 
==========================================================================================================
Cluster Capacity (kb)	-	ceph osd df -f json-pretty|grep "total_kb"|head -n1|awk '{print $2}'
Used Capacity (kb)	-	ceph osd df -f json-pretty|grep "total_kb_used"|awk '{print $2}'
Available Capacity (kb)	-	ceph osd df -f json-pretty|grep "total_kb_avail"|awk '{print $2}'
total_objects (kb)	-	ceph df detail -f json-pretty|grep "total_objects"|awk '{print $2}'
average_utilization (%)	-	ceph osd df -f json-pretty|grep "average_utilization"|awk {'print $2'}

						OSD Stats:
==========================================================================================================
OSD In 			-	ceph -s -f json-pretty|grep "num_in_osds"|head -n1|awk '{print $2}'
OSD Up 			-	ceph -s -f json-pretty|grep "num_up_osds"|head -n1|awk '{print $2}'
OSD Down 		-	sudo ceph osd tree |grep down|wc -l
OSD Out			-	
Average PGs per OSD	-	ceph osd utilization|head -n1|awk '{print $2}'

					Cluster Performance Stats:
==========================================================================================================
Read kB/s		-	ceph -s -f json-pretty|grep "read_bytes_sec"|awk '{print $2}'
Write kB/s		- 	ceph -s -f json-pretty|grep "write_bytes_sec"|awk '{print $2}'
read_op_per_sec		- 	ceph -s -f json-pretty|grep "read_op_per_sec"|awk '{print $2}'
write_op_per_sec	- 	ceph -s -f json-pretty|grep "write_op_per_sec"|awk '{print $2}'

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Useful Link: 
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
https://tracker.ceph.com/projects/ceph/wiki/Guides
